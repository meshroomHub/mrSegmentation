__version__ = "0.3"

import os
from pathlib import Path

from meshroom.core import desc
from meshroom.core.utils import VERBOSE_LEVEL

class Sam3NodeSize(desc.MultiDynamicNodeSize):
    def computeSize(self, node):
        if node.attribute(self._params[0]).isLink:
            return node.attribute(self._params[0]).inputLink.node.size

        from pathlib import Path

        input_path_param = node.attribute(self._params[0])
        extension_param = node.attribute(self._params[1])
        input_path = input_path_param.value
        extension = extension_param.value
        include_suffixes = [extension.lower(), extension.upper()]

        size = 1
        if Path(input_path).is_dir():
            import itertools
            image_paths = list(itertools.chain(*(Path(input_path).glob(f'*.{suffix}') for suffix in include_suffixes)))
            size = len(image_paths)
        
        return size
        
class Sam3(desc.Node):
    size = Sam3NodeSize(['input', 'extensionIn'])
    gpu = desc.Level.INTENSIVE
    parallelization = desc.Parallelization(blockSize=50)

    category = "Utils"
    documentation = """
Based on the Segment Anything model 3, the node generates a binary mask from a set of bounding boxes.
The bounding boxes can be provided through a json file as the one generated by the imageDetectionPrompt node or through a nk file storing a Tracker4 Nuke tracker.

The only requirement for the tracker file is about the tracks names. To be considered, a track name must contain at least one of the following patterns:
 * "In_", "Out_", "bboxT_" or "bboxS".
 * "In_": The track curve will be considered as Clicks inside the object to segment.
 * "Out_": The track curve will be considered as Clicks outside the object to segment.
 * "bboxT_": The tracking bounding box will be used.
 * "bboxS_": The searching bounding box will be used.

The bounding boxes coming from the Nuke tracker are appended to the ones coming from the json file if any.
In case neither tracker nor json file is available, the model is applied on the full image.
In case bounding boxes and clicks are provided, only clicks inside bounding boxes are considered.
"""

    inputs = [
        desc.File(
            name="input",
            label="Input",
            description="Folder or SfMData file.",
            value="",
        ),
        desc.ChoiceParam(
            name="extensionIn",
            label="Input File Extension",
            description="Input image file extension.\n"
                        "Considered only if input is a folder.",
            value="exr",
            values=["exr", "png", "jpg"],
            exclusive=True,
            group="",  # remove from command line params
            enabled=lambda node: Path(node.input.value).is_dir(),
        ),
        desc.StringParam(
            name="prompt",
            label="Prompt",
            description="What to segment, separated by point or one item per line.",
            value="person",
            semantic="multiline",
        ),
        desc.File(
            name="bboxFolder",
            label="BBoxes Folder",
            description="JSON file containing prompting bounding boxes.",
            value="",
        ),
        desc.File(
            name="nukeTracker",
            label="Nuke Tracker",
            description="Nuke file .nk containing a tracker node.",
            value="",
        ),
        desc.File(
            name="segmentationModelPath",
            label="Segmentation Model",
            description="Weights file for the segmentation model.",
            value="${RDS_SAM3_MODEL_PATH}",
        ),
        desc.BoolParam(
            name="maskInvert",
            label="Invert Masks",
            description="Invert mask values. If selected, the pixels corresponding to the mask will be set to 0 instead of 255.",
            value=False,
        ),
        desc.BoolParam(
            name="useGpu",
            label="Use GPU",
            description="Use GPU for computation if available.",
            value=True,
            invalidate=False,
        ),
        desc.BoolParam(
            name="keepFilename",
            label="Keep Filename",
            description="Keep the filename of the inputs for the outputs.",
            value=False,
            enabled=lambda node: not Path(node.input.value).is_dir(),
        ),
        desc.ChoiceParam(
            name="extensionOut",
            label="Output File Extension",
            description="Output image file extension.\n"
                        "If unset, the output file extension will match the input's if possible.",
            value="exr",
            values=["exr", "png", "jpg"],
            exclusive=True,
            group="",  # remove from command line params
        ),
        desc.ChoiceParam(
            name="verboseLevel",
            label="Verbose Level",
            description="Verbosity level (fatal, error, warning, info, debug).",
            value="info",
            values=VERBOSE_LEVEL,
            exclusive=True,
        ),
    ]

    outputs = [
        desc.File(
            name="output",
            label="Masks Folder",
            description="Output path for the masks.",
            value="{nodeCacheFolder}",
        ),
        desc.File(
            name="masks",
            label="Masks",
            description="Generated segmentation masks.",
            semantic="image",
            value=lambda attr: "{nodeCacheFolder}/" + ("<FILESTEM>" if attr.node.keepFilename.value else "<VIEW_ID>") + "." + attr.node.extensionOut.value,
            group="",
        ),
    ]

    def resolvedPaths(self, input_path, extensionIn, outDir, keepFilename, extensionOut):
        from pyalicevision import sfmData
        from pyalicevision import sfmDataIO
        from pathlib import Path
        import itertools

        include_suffixes = [extensionIn.lower(), extensionIn.upper()]
        paths = {}
        if Path(input_path).is_dir():
            input_filepaths = sorted(itertools.chain(*(Path(input_path).glob(f'*.{suffix}') for suffix in include_suffixes)))
            for frameId, inputFile in enumerate(input_filepaths):
                outputFileMask = os.path.join(outDir, Path(inputFile).stem + "." + extensionOut)
                outputFileBoxes = os.path.join(outDir, "bboxes_" + Path(inputFile).stem + ".jpg")
                paths[str(inputFile)] = (outputFileMask, outputFileBoxes, frameId)
        elif Path(input_path).suffix.lower() in [".sfm", ".abc"]:
            if Path(input_path).exists():
                dataAV = sfmData.SfMData()
                if sfmDataIO.load(dataAV, input_path, sfmDataIO.ALL) and os.path.isdir(outDir):
                    views = dataAV.getViews()
                    for id, v in views.items():
                        inputFile = v.getImage().getImagePath()
                        frameId = v.getFrameId()
                        if keepFilename:
                            outputFileMask = os.path.join(outDir, Path(inputFile).stem + "." + extensionOut)
                            outputFileBoxes = os.path.join(outDir, "bboxes_" + Path(inputFile).stem + ".jpg")
                        else:
                            outputFileMask = os.path.join(outDir, str(id) + "." + extensionOut)
                            outputFileBoxes = os.path.join(outDir, "bboxes_" + str(id) + ".jpg")
                        paths[inputFile] = (outputFileMask, outputFileBoxes, frameId)

        return paths

    def processChunk(self, chunk):
        import json
        from segmentationRDS import image #, segmentation, nktracker
        from sam3.model_builder import build_sam3_image_model
        from sam3.model.sam3_image_processor import Sam3Processor
        import numpy as np
        import torch
        from pyalicevision import image as avimg

        processor = None
        try:
            chunk.logManager.start(chunk.node.verboseLevel.value)

            if not chunk.node.input:
                chunk.logger.warning("Nothing to segment")
                return
            if not chunk.node.output.value:
                return
            if not chunk.node.nukeTracker.value and not chunk.node.bboxFolder.value and not chunk.node.prompt.value:
                chunk.logger.warning("No Prompt, no bounding boxes and no tracker info, the full image will be segmented")
            if chunk.node.nukeTracker.value:
                tracker = nktracker.nkTracker(chunk.node.nukeTracker.value)
            else:
                tracker = None

            chunk.logger.info("Chunk range from {} to {}".format(chunk.range.start, chunk.range.last))

            outFiles = self.resolvedPaths(chunk.node.input.value, chunk.node.extensionIn.value, chunk.node.output.value, chunk.node.keepFilename.value, chunk.node.extensionOut.value)

            if not os.path.exists(chunk.node.output.value):
                os.mkdir(chunk.node.output.value)

            device = "cuda" if torch.cuda.is_available() else "cpu"
            model = build_sam3_image_model(checkpoint_path=chunk.node.segmentationModelPath.evalValue, device=device)
            processor = Sam3Processor(model)

            bboxDict = {}
            if chunk.node.bboxFolder.value:
                for file in os.listdir(chunk.node.bboxFolder.value):
                    if file.endswith(".json"):
                        with open(os.path.join(chunk.node.bboxFolder.value,file)) as bboxFile:
                            bb = json.load(bboxFile)
                            bboxDict.update(bb)

            metadata_deep_model = {}
            metadata_deep_model["Meshroom:mrSegmentation:DeepModelName"] = "SegmentAnything"
            metadata_deep_model["Meshroom:mrSegmentation:DeepModelVersion"] = "sam3"

            for k, (iFile, oFile) in enumerate(outFiles.items()):
                if k >= chunk.range.start and k <= chunk.range.last:
                    img, h_ori, w_ori, PAR, orientation = image.loadImage(iFile, True)
                    frameId = oFile[2]

                    chunk.logger.info("frameId: {} - {}".format(frameId, iFile))

                    bboxes = []
                    if not chunk.node.nukeTracker.value and not chunk.node.bboxFolder.value and not chunk.node.prompt.value:
                        bboxes = [[0, 0, img.shape[1] - 1, img.shape[0] - 1]]
                    elif chunk.node.bboxFolder.value:
                        bboxes = bboxDict[iFile]["bboxes"]

                    clicksIn = []
                    clicksOut = []
                    if tracker is not None:
                        data = tracker.getDataAtFrame(frameId, img.shape[0], PAR)
                        trackNames = tracker.getTrackNames()
                        for trackName in trackNames:
                            if data[trackName][0][0] is not None:
                                if trackName.find("In_") != -1:
                                    clicksIn.append(data[trackName][0])
                                elif trackName.find("Out_") != -1:
                                    clicksOut.append(data[trackName][0])
                                if trackName.find("bboxT_") != -1:
                                    bboxes.append(data[trackName][1])
                                if trackName.find("bboxS_") != -1:
                                    bboxes.append(data[trackName][2])

                    chunk.logger.debug("Clicks In: {}".format(clicksIn))
                    chunk.logger.debug("Clicks Out: {}".format(clicksOut))
                    chunk.logger.debug("bboxes: {}".format(bboxes))

                    imgTensor = torch.from_numpy(np.transpose((255.0*img).astype(np.uint8), (2, 0, 1)))
                    inference_state = processor.set_image(imgTensor)
                    # Prompt the model with text
                    output = processor.set_text_prompt(state=inference_state, prompt=chunk.node.prompt.value)
                    # Get the masks, bounding boxes, and scores
                    masks, boxes, scores = output["masks"], output["boxes"], output["scores"]

                    mask_image = np.zeros_like(img)
                    for idx in range(len(masks)):
                        m = masks[idx].squeeze(0).cpu()
                        mask_image[m] = [255, 255, 255]

                    if chunk.node.maskInvert.value:
                        mask = (mask_image[:,:,0:1] == 0).astype('float32')
                    else:
                        mask = (mask_image[:,:,0:1] > 0).astype('float32')

                    optWrite = avimg.ImageWriteOptions()
                    if Path(oFile[0]).suffix.lower() == ".exr":
                        optWrite.toColorSpace(avimg.EImageColorSpace_NO_CONVERSION)
                        optWrite.exrCompressionMethod(avimg.EImageExrCompression_stringToEnum("DWAA"))
                        optWrite.exrCompressionLevel(300)
                    else:
                        optWrite.toColorSpace(avimg.EImageColorSpace_SRGB)

                    image.writeImage(oFile[0], mask, h_ori, w_ori, orientation, PAR, metadata_deep_model, optWrite)

        finally:
            del processor
            torch.cuda.empty_cache()
            chunk.logManager.end()
